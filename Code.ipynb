{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    Books are valuble materials for humans. Throughout history, the books industry has evolved from hand written books, to mechanical typing machines, and now to the age of electronic, and audio books. Amazon, Google and many other companies are investing in this field. As the books industry evolved, the number of books has increased dramatically giving the readers very wide variaty of books materials and many options. However, this also put the readers in so much confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    To make it easier for readers it is very important to classify the books accuratly depending on thier content. Each single book must be labeled according to the title and the description of the book. This process of assigning books to different generes when done manually, can be very tidues, time consuming and costy. <br>\n",
    "    \n",
    "    \n",
    "   Machine Learning with the aid of Natural Language Processing (NLP), can help automating this process and thus, saving the companies both time and money.\n",
    "   \n",
    "   In this project our aim is to design a classifier that can assign genere to each book, given their title and description. NLP would be used to process the title and the description of each book before using a Naive Bayes classifier to classify the books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, StemmerI, PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('google_books_1299.csv')\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>voters</th>\n",
       "      <th>price</th>\n",
       "      <th>currency</th>\n",
       "      <th>description</th>\n",
       "      <th>publisher</th>\n",
       "      <th>page_count</th>\n",
       "      <th>generes</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>language</th>\n",
       "      <th>published_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Attack on Titan: Volume 13</td>\n",
       "      <td>Hajime Isayama</td>\n",
       "      <td>4.6</td>\n",
       "      <td>428</td>\n",
       "      <td>43.28</td>\n",
       "      <td>SAR</td>\n",
       "      <td>NO SAFE PLACE LEFT At great cost to the Garris...</td>\n",
       "      <td>Kodansha Comics</td>\n",
       "      <td>192</td>\n",
       "      <td>none</td>\n",
       "      <td>9781612626864</td>\n",
       "      <td>English</td>\n",
       "      <td>Jul 31, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antiques Roadkill: A Trash 'n' Treasures Mystery</td>\n",
       "      <td>Barbara Allan</td>\n",
       "      <td>3.3</td>\n",
       "      <td>23</td>\n",
       "      <td>26.15</td>\n",
       "      <td>SAR</td>\n",
       "      <td>Determined to make a new start in her quaint h...</td>\n",
       "      <td>Kensington Publishing Corp.</td>\n",
       "      <td>288</td>\n",
       "      <td>Fiction , Mystery &amp;amp, Detective , Cozy , Gen...</td>\n",
       "      <td>9780758272799</td>\n",
       "      <td>English</td>\n",
       "      <td>Jul 1, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Art of Super Mario Odyssey</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>3.9</td>\n",
       "      <td>9</td>\n",
       "      <td>133.85</td>\n",
       "      <td>SAR</td>\n",
       "      <td>Take a globetrotting journey all over the worl...</td>\n",
       "      <td>Dark Horse Comics</td>\n",
       "      <td>368</td>\n",
       "      <td>Games &amp;amp, Activities , Video &amp;amp, Electronic</td>\n",
       "      <td>9781506713816</td>\n",
       "      <td>English</td>\n",
       "      <td>Nov 5, 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Getting Away Is Deadly: An Ellie Avery Mystery</td>\n",
       "      <td>Sara Rosett</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>26.15</td>\n",
       "      <td>SAR</td>\n",
       "      <td>With swollen feet and swelling belly, pregnant...</td>\n",
       "      <td>Kensington Publishing Corp.</td>\n",
       "      <td>320</td>\n",
       "      <td>none</td>\n",
       "      <td>9781617734076</td>\n",
       "      <td>English</td>\n",
       "      <td>Mar 1, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Painted Man (The Demon Cycle, Book 1)</td>\n",
       "      <td>Peter V. Brett</td>\n",
       "      <td>4.5</td>\n",
       "      <td>577</td>\n",
       "      <td>28.54</td>\n",
       "      <td>SAR</td>\n",
       "      <td>The stunning debut fantasy novel from author P...</td>\n",
       "      <td>HarperCollins UK</td>\n",
       "      <td>544</td>\n",
       "      <td>Fiction , Fantasy , Dark Fantasy</td>\n",
       "      <td>9780007287758</td>\n",
       "      <td>English</td>\n",
       "      <td>Jan 8, 2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title          author  rating  \\\n",
       "0                        Attack on Titan: Volume 13  Hajime Isayama     4.6   \n",
       "1  Antiques Roadkill: A Trash 'n' Treasures Mystery   Barbara Allan     3.3   \n",
       "2                    The Art of Super Mario Odyssey        Nintendo     3.9   \n",
       "3    Getting Away Is Deadly: An Ellie Avery Mystery     Sara Rosett     4.0   \n",
       "4         The Painted Man (The Demon Cycle, Book 1)  Peter V. Brett     4.5   \n",
       "\n",
       "  voters   price currency                                        description  \\\n",
       "0    428   43.28      SAR  NO SAFE PLACE LEFT At great cost to the Garris...   \n",
       "1     23   26.15      SAR  Determined to make a new start in her quaint h...   \n",
       "2      9  133.85      SAR  Take a globetrotting journey all over the worl...   \n",
       "3     10   26.15      SAR  With swollen feet and swelling belly, pregnant...   \n",
       "4    577   28.54      SAR  The stunning debut fantasy novel from author P...   \n",
       "\n",
       "                     publisher  page_count  \\\n",
       "0              Kodansha Comics         192   \n",
       "1  Kensington Publishing Corp.         288   \n",
       "2            Dark Horse Comics         368   \n",
       "3  Kensington Publishing Corp.         320   \n",
       "4             HarperCollins UK         544   \n",
       "\n",
       "                                             generes           ISBN language  \\\n",
       "0                                               none  9781612626864  English   \n",
       "1  Fiction , Mystery &amp, Detective , Cozy , Gen...  9780758272799  English   \n",
       "2    Games &amp, Activities , Video &amp, Electronic  9781506713816  English   \n",
       "3                                               none  9781617734076  English   \n",
       "4                   Fiction , Fantasy , Dark Fantasy  9780007287758  English   \n",
       "\n",
       "  published_date  \n",
       "0   Jul 31, 2014  \n",
       "1    Jul 1, 2007  \n",
       "2    Nov 5, 2019  \n",
       "3    Mar 1, 2009  \n",
       "4    Jan 8, 2009  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1299 entries, 0 to 1298\n",
      "Data columns (total 13 columns):\n",
      "title             1299 non-null object\n",
      "author            1299 non-null object\n",
      "rating            1224 non-null float64\n",
      "voters            1224 non-null object\n",
      "price             1299 non-null float64\n",
      "currency          1299 non-null object\n",
      "description       1296 non-null object\n",
      "publisher         1299 non-null object\n",
      "page_count        1299 non-null int64\n",
      "generes           1299 non-null object\n",
      "ISBN              1299 non-null object\n",
      "language          1299 non-null object\n",
      "published_date    1299 non-null object\n",
      "dtypes: float64(2), int64(1), object(10)\n",
      "memory usage: 132.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Descreption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|Type|Description|\n",
    "|---|---|---|\n",
    "|title|object|The title of the book|,\n",
    "|author|object|The author of the book|,\n",
    "|rating|float|The rating of the book by the customers|,\n",
    "|voters|int|The number of customers who voted|,\n",
    "|price|float|The book price on Google store|,\n",
    "|currency|object|The currency in which the price is reported|,\n",
    "|description|object|A breif description of the book given in the store|,\n",
    "|publisher|object|The publisher of the book|,\n",
    "|page_count|int|The number of pages of the book|,\n",
    "|generes|object|The generes under which the book is catogrized|,\n",
    "|ISBN|object|the unique identification of each book|\n",
    "|language|object|The language in which the book is written|\n",
    "|published_date|object|The date at which the book was published|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>price</th>\n",
       "      <th>page_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1224.000000</td>\n",
       "      <td>1299.000000</td>\n",
       "      <td>1.299000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.424755</td>\n",
       "      <td>45.882733</td>\n",
       "      <td>1.505974e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.392489</td>\n",
       "      <td>53.588444</td>\n",
       "      <td>3.836540e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.200000</td>\n",
       "      <td>22.470000</td>\n",
       "      <td>1.940000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>38.330000</td>\n",
       "      <td>3.040000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.700000</td>\n",
       "      <td>59.030000</td>\n",
       "      <td>4.160000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>886.080000</td>\n",
       "      <td>9.781302e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rating        price    page_count\n",
       "count  1224.000000  1299.000000  1.299000e+03\n",
       "mean      4.424755    45.882733  1.505974e+10\n",
       "std       0.392489    53.588444  3.836540e+11\n",
       "min       2.500000     0.000000  7.000000e+00\n",
       "25%       4.200000    22.470000  1.940000e+02\n",
       "50%       4.500000    38.330000  3.040000e+02\n",
       "75%       4.700000    59.030000  4.160000e+02\n",
       "max       5.000000   886.080000  9.781302e+12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have some duplicates so we will drop them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(948, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our target is to predict two generes for each book, so we will create two new columns for each row**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_genere'] = df.generes.str.split(',',expand=True)[0]\n",
    "df['second_genere'] = df.generes.str.split(',',expand=True)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will create a new column that incudes both the title and the description of each book**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"title\"].map(str) + ', ' + df[\"description\"].str.strip('\\n\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>voters</th>\n",
       "      <th>price</th>\n",
       "      <th>currency</th>\n",
       "      <th>description</th>\n",
       "      <th>publisher</th>\n",
       "      <th>page_count</th>\n",
       "      <th>generes</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>language</th>\n",
       "      <th>published_date</th>\n",
       "      <th>first_genere</th>\n",
       "      <th>second_genere</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Attack on Titan: Volume 13</td>\n",
       "      <td>Hajime Isayama</td>\n",
       "      <td>4.6</td>\n",
       "      <td>428</td>\n",
       "      <td>43.28</td>\n",
       "      <td>SAR</td>\n",
       "      <td>NO SAFE PLACE LEFT At great cost to the Garris...</td>\n",
       "      <td>Kodansha Comics</td>\n",
       "      <td>192</td>\n",
       "      <td>none</td>\n",
       "      <td>9781612626864</td>\n",
       "      <td>English</td>\n",
       "      <td>Jul 31, 2014</td>\n",
       "      <td>none</td>\n",
       "      <td>None</td>\n",
       "      <td>Attack on Titan: Volume 13, NO SAFE PLACE LEFT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antiques Roadkill: A Trash 'n' Treasures Mystery</td>\n",
       "      <td>Barbara Allan</td>\n",
       "      <td>3.3</td>\n",
       "      <td>23</td>\n",
       "      <td>26.15</td>\n",
       "      <td>SAR</td>\n",
       "      <td>Determined to make a new start in her quaint h...</td>\n",
       "      <td>Kensington Publishing Corp.</td>\n",
       "      <td>288</td>\n",
       "      <td>Fiction , Mystery &amp;amp, Detective , Cozy , Gen...</td>\n",
       "      <td>9780758272799</td>\n",
       "      <td>English</td>\n",
       "      <td>Jul 1, 2007</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Mystery &amp;amp</td>\n",
       "      <td>Antiques Roadkill: A Trash 'n' Treasures Myste...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title          author  rating  \\\n",
       "0                        Attack on Titan: Volume 13  Hajime Isayama     4.6   \n",
       "1  Antiques Roadkill: A Trash 'n' Treasures Mystery   Barbara Allan     3.3   \n",
       "\n",
       "  voters  price currency                                        description  \\\n",
       "0    428  43.28      SAR  NO SAFE PLACE LEFT At great cost to the Garris...   \n",
       "1     23  26.15      SAR  Determined to make a new start in her quaint h...   \n",
       "\n",
       "                     publisher  page_count  \\\n",
       "0              Kodansha Comics         192   \n",
       "1  Kensington Publishing Corp.         288   \n",
       "\n",
       "                                             generes           ISBN language  \\\n",
       "0                                               none  9781612626864  English   \n",
       "1  Fiction , Mystery &amp, Detective , Cozy , Gen...  9780758272799  English   \n",
       "\n",
       "  published_date first_genere  second_genere  \\\n",
       "0   Jul 31, 2014         none           None   \n",
       "1    Jul 1, 2007     Fiction    Mystery &amp   \n",
       "\n",
       "                                                text  \n",
       "0  Attack on Titan: Volume 13, NO SAFE PLACE LEFT...  \n",
       "1  Antiques Roadkill: A Trash 'n' Treasures Myste...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that we have some books with missing geners. Thus we will isolate these books to predict their generes after devoloping our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = df[df.generes=='none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title               0\n",
       "author              0\n",
       "rating             13\n",
       "voters             13\n",
       "price               0\n",
       "currency            0\n",
       "description         3\n",
       "publisher           0\n",
       "page_count          0\n",
       "generes             0\n",
       "ISBN                0\n",
       "language            0\n",
       "published_date      0\n",
       "first_genere        0\n",
       "second_genere     227\n",
       "text                3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((224, 16), (224,))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.dropna(subset=['text'],inplace=True)\n",
    "X_pred = pred_df.text\n",
    "pred_df.shape,X_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The observations that have generes will be saved in another dataframe (train_test_df)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = df[df.generes!='none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have a dataframe with the desired features, we can take X and y variable. Remeber that we need to predict two generes for each book, and therefore, we will have two y's (y1 and y2).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_test_df.text\n",
    "y = train_test_df[['first_genere']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Antiques Roadkill: A Trash 'n' Treasures Myste...\n",
       "2    The Art of Super Mario Odyssey, Take a globetr...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_genere</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Games &amp;amp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_genere\n",
       "1     Fiction \n",
       "2   Games &amp"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegularExpression To Remove the Punctiuation. Inadition to stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessor(text):\n",
    "    text = [re.sub('[^A-Za-z]+n?',' ',x).lower() for x in text]\n",
    "    books = []\n",
    "    for book in text:\n",
    "        book_token = book.split()\n",
    "        book = \" \".join([stem(x.replace(' ','')) for x in book_token])\n",
    "        books.append(book)\n",
    "    text = books\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Antiques Roadkill: A Trash \\'n\\' Treasures Mystery, Determined to make a new start in her quaint hometown on the banks of the Mississippi, Brandy Borne never dreams she\\'ll become the prime suspect in a murder case. . . \\n\\nMoving back in with her eccentric, larger-than-life mother, Brandy Borne finds small-town Serenity anything but serene. It seems an unscrupulous antiques dealer has swindled Vivian out of the family\\'s heirlooms. But when he is found run over in a country lane, Brandy becomes Murder Suspect Number One--with her mother coming in a very close second. . . \\n\\nThe list of other suspects is impressive--the victim\\'s business seems to have been based on bilking seniors out of their possessions. And when the Borne \"girls\" uncover a few very unsavory Serenity secrets, they become targets for a murderer whose favorite hobby seems to be collecting victims. \\n\\nDon\\'t miss Brandy Borne\\'s tips on antiques! \\n\\n\"Cozy mystery fans will love this down-to-earth heroine with the wry sense of humor and a big heart.\" --Nancy Pickard \\n\\n\"With its small-town setting and quirky characters, Antiques Roadkill is fun from start to finish. Dive in and enjoy.\"--Laurien Berenson'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1] # text before Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = text_preprocessor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'antiqu roadkil a trash treasur mysteri determin to make a ew start in her quaint hometown on the bank of the mississippi brandi born ever dream she ll becom the prime suspect in a murder case move back in with her eccentr larger than life mother brandi born find small town seren anyth but seren it seem an unscrupul antiqu dealer has swindl vivian out of the famili s heirloom but when he is found run over in a countri lane brandi becom murder suspect number one with her mother come in a veri close second the list of other suspect is impress the victim s busi seem to have been base on bilk senior out of their possess and when the born girl uncov a few veri unsavori seren secret they becom target for a murder whose favorit hobbi seem to be collect victim don t miss brandi born s tip on antiqu cozi mysteri fan will love this down to earth heroin with the wri sens of humor and a big heart nanci pickard with it small town set and quirki charact antiqu roadkil is fun from start to finish dive in and enjoy laurien berenson'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0] # Text after Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             max_df=0.9,\n",
    "                             max_features=5000, \n",
    "                             min_df=3,\n",
    "                             stop_words= stopwords.words('english'),\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.9, max_features=5000, min_df=3,\n",
       "                ngram_range=(1, 1), preprocessor=None,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vec.toarray() , y , test_size=0.3333 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train1 = y_train.first_genere\n",
    "# y_train2 = y_train.second_genere\n",
    "\n",
    "# y_test1 = y_test.first_genere\n",
    "# y_test2 = y_test.second_genere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abaddon</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abc</th>\n",
       "      <th>abdic</th>\n",
       "      <th>abduct</th>\n",
       "      <th>abductor</th>\n",
       "      <th>abel</th>\n",
       "      <th>abercrombi</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>...</th>\n",
       "      <th>youth</th>\n",
       "      <th>ysann</th>\n",
       "      <th>zaha</th>\n",
       "      <th>zahn</th>\n",
       "      <th>zamyatin</th>\n",
       "      <th>zap</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zeinab</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 4615 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abaddon  abandon  abc  abdic  abduct  abductor  abel  abercrombi  abil  \\\n",
       "0        0        0    0      0       0         0     0           0     1   \n",
       "1        0        0    0      0       0         0     0           0     0   \n",
       "\n",
       "   abl  ...  youth  ysann  zaha  zahn  zamyatin  zap  zealand  zeinab  zero  \\\n",
       "0    0  ...      0      0     0     0         0    0        0       0     0   \n",
       "1    0  ...      0      0     1     0         0    0        0       0     0   \n",
       "\n",
       "   zone  \n",
       "0     0  \n",
       "1     0  \n",
       "\n",
       "[2 rows x 4615 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_train = pd.DataFrame(X_train, columns = vectorizer.get_feature_names())\n",
    "sparse_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test =  vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abaddon</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abc</th>\n",
       "      <th>abdic</th>\n",
       "      <th>abduct</th>\n",
       "      <th>abductor</th>\n",
       "      <th>abel</th>\n",
       "      <th>abercrombi</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>...</th>\n",
       "      <th>youth</th>\n",
       "      <th>ysann</th>\n",
       "      <th>zaha</th>\n",
       "      <th>zahn</th>\n",
       "      <th>zamyatin</th>\n",
       "      <th>zap</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zeinab</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 4615 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abaddon  abandon  abc  abdic  abduct  abductor  abel  abercrombi  abil  \\\n",
       "0        0        0    0      0       0         0     0           0     0   \n",
       "1        0        0    0      0       0         0     0           0     0   \n",
       "\n",
       "   abl  ...  youth  ysann  zaha  zahn  zamyatin  zap  zealand  zeinab  zero  \\\n",
       "0    0  ...      0      0     0     0         0    0        0       0     0   \n",
       "1    0  ...      0      0     0     0         0    0        0       0     0   \n",
       "\n",
       "   zone  \n",
       "0     0  \n",
       "1     0  \n",
       "\n",
       "[2 rows x 4615 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_test = pd.DataFrame(X_test, columns = vectorizer.get_feature_names())\n",
    "sparse_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomiaNB using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = MultinomialNB()\n",
    "# model_nb2 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb.fit(X_train, y_train)\n",
    "# model_nb2.fit(X_train, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85625"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7427385892116183"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nb2.score(X_train, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nb2.score(X_test, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "# rfc2 = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train,y_train)\n",
    "# rfc2.fit(X_train,y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 4615)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8708333333333333"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7095435684647303"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc2.score(X_train,y_train2)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc2.score(X_test, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomiaNB using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb_tfidf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_trans = TfidfTransformer()\n",
    "tfidf_trans.fit(sparse_train)\n",
    "\n",
    "# tfidf_trans2 = TfidfTransformer()\n",
    "# tfidf_trans2.fit(sparse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = tfidf_trans.transform(sparse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = tfidf_trans.transform(sparse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb_tfidf.fit(train_tfidf, y_train)\n",
    "# model_nb2.fit(train_tfidf, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6916666666666667"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb_tfidf.score(train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nb2.score(train_tfidf, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6763485477178424"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb_tfidf.score(test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nb2.score(test_tfidf, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_tfidf = RandomForestClassifier()\n",
    "# rfc2 = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_tfidf.fit(train_tfidf, y_train)\n",
    "# rfc2.fit(X_train,y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8708333333333333"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_tfidf.score(train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6431535269709544"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_tfidf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a pipline with the best model and performing  GridSearchCV on the CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cvec', vectorizer), ('mnb', model_nb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features':[5000, 6000, 7000, 8000,None],\n",
    "    'cvec__min_df':[1,2, 3],\n",
    "    'cvec__max_df':[0.9, 0.95,1.0],\n",
    "    'cvec__ngram_range': [(1, 1), (1, 2)],\n",
    "    'cvec__stop_words' : [stopwords.words(\"english\"), None]\n",
    "                }\n",
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe, pipe_params, cv=3, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   44.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=0.9,\n",
       "                                                        max_features=5000,\n",
       "                                                        min_df=3,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=['i', 'me',\n",
       "                                                                    'my',\n",
       "                                                                    'myself',\n",
       "                                                                    'we...\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'cvec__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                               'our', 'ours', 'ourselves',\n",
       "                                               'you', \"you're\", \"you've\",\n",
       "                                               \"you'll\", \"you'd\", 'your',\n",
       "                                               'yours', 'yourself',\n",
       "                                               'yourselves', 'he', 'him', 'his',\n",
       "                                               'himself', 'she', \"she's\", 'her',\n",
       "                                               'hers', 'herself', 'it', \"it's\",\n",
       "                                               'its', 'itself', ...],\n",
       "                                              None]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cvec',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=0.9,\n",
       "                                 max_features=5000, min_df=3,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...],\n",
       "                                 strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('mnb',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7503467406380028"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The best estimator is the same as the one used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictting the generes for the books with 'none' in the column of genere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = text_preprocessor(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_sparse = vectorizer.transform(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.generes = model_nb.predict(X_pred_sparse.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>voters</th>\n",
       "      <th>price</th>\n",
       "      <th>currency</th>\n",
       "      <th>description</th>\n",
       "      <th>publisher</th>\n",
       "      <th>page_count</th>\n",
       "      <th>generes</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>language</th>\n",
       "      <th>published_date</th>\n",
       "      <th>first_genere</th>\n",
       "      <th>second_genere</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Attack on Titan: Volume 13</td>\n",
       "      <td>Hajime Isayama</td>\n",
       "      <td>4.6</td>\n",
       "      <td>428</td>\n",
       "      <td>43.28</td>\n",
       "      <td>SAR</td>\n",
       "      <td>NO SAFE PLACE LEFT At great cost to the Garris...</td>\n",
       "      <td>Kodansha Comics</td>\n",
       "      <td>192</td>\n",
       "      <td>Comics &amp;amp</td>\n",
       "      <td>9781612626864</td>\n",
       "      <td>English</td>\n",
       "      <td>Jul 31, 2014</td>\n",
       "      <td>none</td>\n",
       "      <td>None</td>\n",
       "      <td>Attack on Titan: Volume 13, NO SAFE PLACE LEFT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Getting Away Is Deadly: An Ellie Avery Mystery</td>\n",
       "      <td>Sara Rosett</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>26.15</td>\n",
       "      <td>SAR</td>\n",
       "      <td>With swollen feet and swelling belly, pregnant...</td>\n",
       "      <td>Kensington Publishing Corp.</td>\n",
       "      <td>320</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>9781617734076</td>\n",
       "      <td>English</td>\n",
       "      <td>Mar 1, 2009</td>\n",
       "      <td>none</td>\n",
       "      <td>None</td>\n",
       "      <td>Getting Away Is Deadly: An Ellie Avery Mystery...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            title          author  rating  \\\n",
       "0                      Attack on Titan: Volume 13  Hajime Isayama     4.6   \n",
       "3  Getting Away Is Deadly: An Ellie Avery Mystery     Sara Rosett     4.0   \n",
       "\n",
       "  voters  price currency                                        description  \\\n",
       "0    428  43.28      SAR  NO SAFE PLACE LEFT At great cost to the Garris...   \n",
       "3     10  26.15      SAR  With swollen feet and swelling belly, pregnant...   \n",
       "\n",
       "                     publisher  page_count      generes           ISBN  \\\n",
       "0              Kodansha Comics         192  Comics &amp  9781612626864   \n",
       "3  Kensington Publishing Corp.         320     Fiction   9781617734076   \n",
       "\n",
       "  language published_date first_genere second_genere  \\\n",
       "0  English   Jul 31, 2014         none          None   \n",
       "3  English    Mar 1, 2009         none          None   \n",
       "\n",
       "                                                text  \n",
       "0  Attack on Titan: Volume 13, NO SAFE PLACE LEFT...  \n",
       "3  Getting Away Is Deadly: An Ellie Avery Mystery...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- In this project a classification model was developed using NLP analysis and Naieve Bayes Models.\n",
    "- The best Accuracy found was 0.7427385892116183 for the test and 0.8708333333333333 for train data. \n",
    "- The score can be enhanced with more data.\n",
    "- We recommend that more books be scraped from google books store to achieve higher accuracy for the model.\n",
    "- It is also recommewnded to build a recommender system and use the NLP analysis to help enhancing the recommendations accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
